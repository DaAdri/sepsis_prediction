{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e94a852",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e76a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import resample # équilibrage de classes\n",
    "\n",
    "# Préparation des données pour entrainement\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# sauvegarde des hyperparamètres\n",
    "import pickle\n",
    "\n",
    "# Explicabilité\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "# models\n",
    "from xgboost import XGBClassifier\n",
    "import optuna # optimisation des hyper paramètres\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, Conv1D, Conv2D, Flatten, MaxPooling2D, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.utils import resample # équilibrage de classes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fc4d0",
   "metadata": {},
   "source": [
    "# Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8af790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, drop_columns=None):\n",
    "    \"\"\"Charge les données depuis un fichier CSV et supprime les colonnes indésirées.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    if drop_columns is not None:\n",
    "        df = df.drop(columns=drop_columns)\n",
    "    return df\n",
    "\n",
    "def display_basic_info(df):\n",
    "    \"\"\"Affiche les informations de base sur le DataFrame, y compris sa forme, ses colonnes,\n",
    "    un résumé descriptif, les valeurs manquantes par colonne et les premières lignes.\"\"\"\n",
    "\n",
    "    print(\"Shape of the DataFrame:\", df.shape)\n",
    "    print('\\nNumber of unique patients:', df['Patient_ID'].nunique())\n",
    "\n",
    "    if 'SepsisLabel' in df.columns:\n",
    "        print(\"\\nSepsisLabel class counts:\\n\", df['SepsisLabel'].value_counts())\n",
    "\n",
    "    if 'will_have_sepsis' in df.columns:\n",
    "        print(\"\\nNumber of unique patients per class in 'will_have_sepsis':\\n\", df.groupby('will_have_sepsis')['Patient_ID'].nunique())\n",
    "\n",
    "    print(\"\\nColumns in the DataFrame:\\n\", df.columns)\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "\n",
    "    print(\"\\nDescriptive Statistics:\\n\", df.describe())\n",
    "\n",
    "    print(\"\\nMissing Values Per Column:\\n\", df.isna().sum())\n",
    "\n",
    "    print(\"\\nFirst 5 Rows of the DataFrame:\\n\", df.head())\n",
    "\n",
    "\n",
    "def sort_dataset_by_patient_and_hour(df, patient_id_column='Patient_ID', hour_column='Hour'):\n",
    "    \"\"\"\n",
    "    Trie le DataFrame en fonction des colonnes spécifiées pour Patient_ID et Hour,\n",
    "    assurant que les données sont triées d'abord par patient, puis par heure pour chaque patient.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame à trier.\n",
    "    patient_id_column (str): Nom de la colonne contenant les identifiants des patients.\n",
    "    hour_column (str): Nom de la colonne contenant les heures des prises de mesures.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame trié selon les identifiants des patients et les heures.\n",
    "    \"\"\"\n",
    "    sorted_df = df.sort_values(by=[patient_id_column, hour_column])\n",
    "    return sorted_df\n",
    "\n",
    "def aggregate_sepsis_label(df, patient_id_column='Patient_ID', sepsis_label_column='SepsisLabel'):\n",
    "    \"\"\"\n",
    "    Agrège les données pour chaque patient pour déterminer si le patient a eu un sepsis\n",
    "    à un moment quelconque et ajoute cette information dans une nouvelle colonne.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    patient_id_column (str): Le nom de la colonne contenant les identifiants des patients.\n",
    "    sepsis_label_column (str): Le nom de la colonne contenant les étiquettes de sepsis.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame enrichi avec une colonne indiquant si le patient a eu un sepsis.\n",
    "    \"\"\"\n",
    "    # Aggrégation des données par patient avec le maximum de SepsisLabel\n",
    "    aggregated_df = df.groupby(patient_id_column)[sepsis_label_column].max().reset_index()\n",
    "\n",
    "    # Renommer la colonne pour clarifier qu'il s'agit du résultat de l'aggrégation\n",
    "    aggregated_df = aggregated_df.rename(columns={sepsis_label_column: 'will_have_sepsis'})\n",
    "\n",
    "    # Joindre avec les données originales pour obtenir un DataFrame complet par patient\n",
    "    aggregated_full_df = df.merge(aggregated_df, on=patient_id_column)\n",
    "\n",
    "    return aggregated_full_df\n",
    "\n",
    "\n",
    "def get_nbr_rows_per_patient_v2(df, time_window):\n",
    "    \"\"\"Récupère 6 lignes pour chaque patient.\n",
    "\n",
    "    - Pour les patients avec 'will_have_sepsis' = 0, prend les  time_window premières lignes.\n",
    "    - Pour les patients avec 'will_have_sepsis' = 1, prend les time_window/2  lignes avant et après le premier 'SepsisLabel' = 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Liste pour stocker les résultats\n",
    "    result = []\n",
    "\n",
    "    # Boucler à travers chaque patient\n",
    "    for patient_id, patient_data in df.groupby('Patient_ID'):\n",
    "        # Vérifier la classe will_have_sepsis\n",
    "        will_have_sepsis = patient_data['will_have_sepsis'].iloc[0]\n",
    "\n",
    "        if will_have_sepsis == 0:\n",
    "            # Prendre les 6 premières lignes pour les patients avec 'will_have_sepsis' = 0\n",
    "            result.append(patient_data.head(time_window))\n",
    "        else:\n",
    "            # Pour les patients avec 'will_have_sepsis' = 1\n",
    "            # Trouver la première ligne où 'SepsisLabel' = 1\n",
    "            sepsis_start = patient_data[patient_data['SepsisLabel'] == 1]\n",
    "\n",
    "            if not sepsis_start.empty:\n",
    "                # Prendre les 6 lignes à partir de la première occurrence de 'SepsisLabel' = 1\n",
    "                start_index = sepsis_start.index[0]\n",
    "                result.append(patient_data.loc[start_index - time_window/2 :start_index + (time_window/2 - 1)])\n",
    "\n",
    "    # Combiner toutes les parties en un seul DataFrame\n",
    "    final_df = pd.concat(result)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df, interest_columns=None, missing_value_threshold=0.3):\n",
    "    \"\"\"Nettoie le DataFrame en supprimant les lignes avec trop de valeurs manquantes.\n",
    "    Si 'interest_columns' n'est pas spécifié, toutes les colonnes sont prises en compte.\"\"\"\n",
    "\n",
    "    if interest_columns is None:\n",
    "        interest_columns = df.columns.tolist()\n",
    "    seuil = missing_value_threshold * len(interest_columns)\n",
    "    cleaned_df = df.dropna(subset=interest_columns, thresh=len(interest_columns) - seuil)\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def check_nbr_rows_per_patient(df, count = 6):\n",
    "    \"\"\"Vérifie si tous les patients ont exactement count lignes.\"\"\"\n",
    "\n",
    "    # Compter le nombre de lignes par patient\n",
    "    row_counts = df.groupby('Patient_ID').size()\n",
    "\n",
    "    # Vérifier si tous les patients ont exactement 6 lignes\n",
    "    all_have_six = (row_counts == count).all()\n",
    "\n",
    "    if all_have_six:\n",
    "        print(\"Tous les patients ont exactement 6 lignes.\")\n",
    "    else:\n",
    "        print(\"Certains patients n'ont pas exactement 6 lignes.\")\n",
    "        # Afficher les patients qui n'ont pas count lignes\n",
    "        print(row_counts[row_counts != count])\n",
    "\n",
    "    return all_have_six\n",
    "\n",
    "def remove_patients_without_nbr_rows(df, count = 6):\n",
    "    \"\"\"Supprime les patients qui n'ont pas exactement count lignes.\"\"\"\n",
    "\n",
    "    # Compter le nombre de lignes par patient\n",
    "    row_counts = df.groupby('Patient_ID').size()\n",
    "\n",
    "    # Trouver les patients qui ont exactement count lignes\n",
    "    valid_patients = row_counts[row_counts == count].index\n",
    "\n",
    "    # Filtrer le DataFrame pour ne garder que les patients valides\n",
    "    filtered_df = df[df['Patient_ID'].isin(valid_patients)]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def add_time_to_sepsis_column(df):\n",
    "    # Trouver le premier instant où chaque patient a SepsisLabel = 1\n",
    "    first_sepsis_time = df[df['SepsisLabel'] == 1].groupby('Patient_ID')['Hour'].min()\n",
    "\n",
    "    # Mapper ces temps de première sepsis sur les patients dans le DataFrame\n",
    "    df['time_to_first_sepsis'] = df['Patient_ID'].map(first_sepsis_time)\n",
    "\n",
    "    # Calculer time_to_sepsis comme la différence entre l'heure de la première sepsis et HospAdmTime\n",
    "    # Note: Assurez-vous que 'HospAdmTime' est bien l'heure d'admission initiale pour chaque patient.\n",
    "    # Si 'HospAdmTime' change par patient et enregistrement, cela devrait être ajusté en conséquence.\n",
    "    df['time_to_sepsis'] = df['time_to_first_sepsis']  # - df['HospAdmTime']\n",
    "\n",
    "    # Supprimer la colonne intermédiaire 'time_to_first_sepsis' si non nécessaire\n",
    "    df.drop(columns=['time_to_first_sepsis'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def balance_classes(df, target_column, method='undersample', random_state=123):\n",
    "    \"\"\"\n",
    "    Équilibre les classes dans un DataFrame en sous-échantillonnant la classe majoritaire ou\n",
    "    en sur-échantillonnant la classe minoritaire selon le paramètre 'method'.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame à équilibrer.\n",
    "    target_column (str): Nom de la colonne contenant les étiquettes de classe.\n",
    "    method (str): Méthode d'équilibrage, 'undersample' pour sous-échantillonnage ou 'oversample' pour sur-échantillonnage.\n",
    "    random_state (int): Graine pour la génération de nombres aléatoires pour la reproductibilité.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame où les classes sont équilibrées.\n",
    "    \"\"\"\n",
    "    # Identifier les classes majoritaire et minoritaire\n",
    "    class_counts = df[target_column].value_counts()\n",
    "    major_class_label = class_counts.idxmax()\n",
    "    minor_class_label = class_counts.idxmin()\n",
    "\n",
    "    major_class = df[df[target_column] == major_class_label]\n",
    "    minor_class = df[df[target_column] == minor_class_label]\n",
    "\n",
    "    if method == 'undersample':\n",
    "        # Sous-échantillonnage de la classe majoritaire\n",
    "        resampled_major_class = resample(major_class,\n",
    "                                         replace=False,\n",
    "                                         n_samples=len(minor_class),\n",
    "                                         random_state=random_state)\n",
    "        balanced_df = pd.concat([resampled_major_class, minor_class])\n",
    "    elif method == 'oversample':\n",
    "        # Sur-échantillonnage de la classe minoritaire\n",
    "        resampled_minor_class = resample(minor_class,\n",
    "                                         replace=True,\n",
    "                                         n_samples=len(major_class),\n",
    "                                         random_state=random_state)\n",
    "        balanced_df = pd.concat([major_class, resampled_minor_class])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "def balance_data_by_sepsis_label(df):\n",
    "    \"\"\"\n",
    "    Équilibre les données en sélectionnant le même nombre de patients ayant 0 et 1 comme valeur de 'will_have_sepsis'.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame équilibré avec un nombre égal de patients ayant 0 et 1 comme valeur de 'will_have_sepsis'.\n",
    "    \"\"\"\n",
    "    # Séparer les patients ayant 0 et 1 comme valeur de 'will_have_sepsis'\n",
    "    sepsis_positive = df[df['will_have_sepsis'] == 1]\n",
    "    sepsis_negative = df[df['will_have_sepsis'] == 0]\n",
    "\n",
    "    # Trouver le nombre minimal de patients dans les deux groupes\n",
    "    min_count = min(len(sepsis_positive), len(sepsis_negative))\n",
    "\n",
    "    # Échantillonner de manière aléatoire un nombre égal de patients de chaque groupe\n",
    "    sepsis_positive_sample = sepsis_positive.sample(n=min_count, random_state=42)\n",
    "    sepsis_negative_sample = sepsis_negative.sample(n=min_count, random_state=42)\n",
    "\n",
    "    # Combiner les échantillons pour créer un DataFrame équilibré\n",
    "    balanced_df = pd.concat([sepsis_positive_sample, sepsis_negative_sample])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "def balance_classes_by_nan(df):\n",
    "    \"\"\"Rééquilibre les classes en prenant tous les patients qui auront le sepsis\n",
    "    et en sélectionnant le même nombre de patients non atteints ayant le moins de NaN.\"\"\"\n",
    "\n",
    "    # Séparer les patients en fonction de 'will_have_sepsis'\n",
    "    sepsis_patients = df[df['will_have_sepsis'] == 1]\n",
    "    non_sepsis_patients = df[df['will_have_sepsis'] == 0]\n",
    "\n",
    "    # Calculer le pourcentage de NaN pour chaque patient non atteint\n",
    "    nan_percentage_per_patient = non_sepsis_patients.isna().mean(axis=1)\n",
    "\n",
    "    # Associer le pourcentage de NaN à chaque 'Patient_ID' en utilisant .loc pour éviter l'avertissement\n",
    "    non_sepsis_patients = non_sepsis_patients.copy()  # To avoid the warning\n",
    "    non_sepsis_patients['nan_percentage'] = nan_percentage_per_patient\n",
    "\n",
    "    # Sélectionner les patients non atteints avec le moins de valeurs manquantes\n",
    "    selected_non_sepsis_patients = non_sepsis_patients.groupby('Patient_ID').mean().sort_values(by='nan_percentage').head(sepsis_patients['Patient_ID'].nunique()).index\n",
    "\n",
    "    # Filtrer le DataFrame pour obtenir les patients non atteints sélectionnés\n",
    "    balanced_non_sepsis_patients = non_sepsis_patients[non_sepsis_patients['Patient_ID'].isin(selected_non_sepsis_patients)]\n",
    "\n",
    "    # Combiner avec les patients atteints de sepsis\n",
    "    balanced_df = pd.concat([sepsis_patients, balanced_non_sepsis_patients])\n",
    "\n",
    "    # Supprimer la colonne temporaire 'nan_percentage'\n",
    "    balanced_df = balanced_df.drop(columns=['nan_percentage'], errors='ignore')\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "\n",
    "def filter_rows_by_time_to_sepsis(df, time_window = 24):\n",
    "    \"\"\"\n",
    "    Filtre les lignes du DataFrame pour conserver uniquement celles où 'time_to_sepsis' est NaN,\n",
    "    ou celles où 'Hour' est compris entre 'time_to_sepsis + HospAdmTime' et 'time_to_sepsis + HospAdmTime - 24'.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    time_window (int) : fenêtre temporelle à récupérer\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame filtré avec les lignes désirées.\n",
    "    \"\"\"\n",
    "    # Filtrer les lignes où 'time_to_sepsis' est NaN\n",
    "    nan_time_to_sepsis_df = df[df['time_to_sepsis'].isna()]\n",
    "\n",
    "    # Filtrer les lignes où 'Hour' est dans l'intervalle spécifié\n",
    "    non_nan_time_to_sepsis_df = df.dropna(subset=['time_to_sepsis'])\n",
    "    filtered_df = non_nan_time_to_sepsis_df[\n",
    "        (non_nan_time_to_sepsis_df['Hour'] >= non_nan_time_to_sepsis_df['time_to_sepsis'] + non_nan_time_to_sepsis_df['HospAdmTime'] - time_window) &\n",
    "        (non_nan_time_to_sepsis_df['Hour'] <= non_nan_time_to_sepsis_df['time_to_sepsis'] + non_nan_time_to_sepsis_df['HospAdmTime'])\n",
    "    ]\n",
    "\n",
    "    # Combiner les DataFrames filtrés\n",
    "    combined_df = pd.concat([nan_time_to_sepsis_df, filtered_df])\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def filter_rows_by_time_to_sepsis_min_max(df, min_time=6, max_time=12):\n",
    "    \"\"\"\n",
    "    Filtre les lignes du DataFrame pour conserver uniquement celles où 'time_to_sepsis' est NaN,\n",
    "    ou entre un minimum et un maximum spécifié (inclus).\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    min_time (int): La valeur minimale de 'time_to_sepsis' pour conserver la ligne.\n",
    "    max_time (int): La valeur maximale de 'time_to_sepsis' pour conserver la ligne.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame filtré avec les lignes désirées.\n",
    "    \"\"\"\n",
    "    # Filtrer le DataFrame pour conserver les lignes où 'time_to_sepsis' est NaN ou dans l'intervalle spécifié\n",
    "    filtered_df = df[(df['time_to_sepsis'].isna()) |\n",
    "                     ((df['time_to_sepsis'] >= min_time) & (df['time_to_sepsis'] <= max_time))]\n",
    "    return filtered_df\n",
    "\n",
    "def filter_rows_by_time_to_sepsis_min(df, min_time=6):\n",
    "    \"\"\"\n",
    "    Filtre les lignes du DataFrame pour conserver uniquement celles où 'time_to_sepsis' est NaN,\n",
    "    ou entre un minimum et un maximum spécifié (inclus).\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    min_time (int): La valeur minimale de 'time_to_sepsis' pour conserver la ligne.\n",
    "    max_time (int): La valeur maximale de 'time_to_sepsis' pour conserver la ligne.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Un DataFrame filtré avec les lignes désirées.\n",
    "    \"\"\"\n",
    "    # Filtrer le DataFrame pour conserver les lignes où 'time_to_sepsis' est NaN ou dans l'intervalle spécifié\n",
    "    filtered_df = df[(df['time_to_sepsis'].isna()) | (df['time_to_sepsis'] >= min_time)]\n",
    "    return filtered_df\n",
    "\n",
    "def add_sepsis_label_12(df):\n",
    "    \"\"\"\n",
    "    Ajoute une colonne 'SepsisLabel_12' au DataFrame, qui a les mêmes valeurs que 'will_have_sepsis'\n",
    "    sauf pour les lignes où 'Hour' < ('time_to_sepsis' - 12), qui auront la valeur 0, tout en gérant les valeurs NaN.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Le DataFrame avec la nouvelle colonne 'SepsisLabel_12'.\n",
    "    \"\"\"\n",
    "    # Copier la colonne 'will_have_sepsis' dans 'SepsisLabel_12'\n",
    "    df['SepsisLabel_12'] = df['will_have_sepsis']\n",
    "\n",
    "    # Mettre à jour les valeurs de 'SepsisLabel_12' en fonction de la condition\n",
    "    condition = (df['time_to_sepsis'].notna()) & (df['Hour'] < (df['time_to_sepsis'] - 12))\n",
    "    df.loc[condition, 'SepsisLabel_12'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "###################  Préparation de l'ensemble d'entrainement et de test #########################\n",
    "#################################################################################################\n",
    "\n",
    "def prepare_train_test(df, label_column, test_size=0.2, random_state=None, stratify=True):\n",
    "    \"\"\"\n",
    "    Prépare les ensembles d'entraînement et de test à partir d'un DataFrame donné.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame à partir duquel les ensembles doivent être créés.\n",
    "    label_column (str): Le nom de la colonne qui contient les étiquettes cibles.\n",
    "    test_size (float): La proportion du dataset à inclure dans l'ensemble de test.\n",
    "    random_state (int): Contrôle la reproductibilité des résultats en fixant un seed pour le générateur aléatoire.\n",
    "    stratify (bool): Si True, les données sont divisées de façon à préserver le même pourcentage pour chaque classe cible dans les ensembles de train et de test.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Contient les ensembles X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    # Séparation des features et des étiquettes\n",
    "    X = df.drop(columns=[label_column])\n",
    "    y = df[label_column]\n",
    "\n",
    "    # Stratification optionnelle basée sur les étiquettes\n",
    "    stratify_param = y if stratify else None\n",
    "\n",
    "    # Répartition des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
    "                                                        stratify=stratify_param, random_state=random_state)\n",
    "\n",
    "    # Affichage des dimensions des ensembles pour vérification\n",
    "    print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape, \"y_train:\", y_train.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def split_train_test_data(df, test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Sépare les données en ensembles d'entraînement et de test, en s'assurant que les patients\n",
    "    avec et sans sepsis sont correctement répartis sans chevauchement entre les ensembles.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    test_size (float): La proportion de chaque groupe de patients à utiliser pour le test.\n",
    "    random_seed (int): La graine pour la génération de nombres aléatoires pour la reproductibilité.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Un tuple contenant deux DataFrames, (train_df, test_df).\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)  # Pour la reproductibilité\n",
    "\n",
    "    # Identifier les patients qui ont eu un sepsis\n",
    "    patients_with_sepsis = df[df['will_have_sepsis'] == 1]['Patient_ID'].unique()\n",
    "\n",
    "    # Sélectionner une proportion pour le test parmi les patients avec sepsis\n",
    "    test_patients_with_sepsis = np.random.choice(patients_with_sepsis, size=int(len(patients_with_sepsis) * test_size), replace=False)\n",
    "\n",
    "    # Identifier et sélectionner une proportion pour le test parmi les patients sans sepsis\n",
    "    patients_without_sepsis = df[df['will_have_sepsis'] == 0]['Patient_ID'].unique()\n",
    "    test_patients_without_sepsis = np.random.choice(patients_without_sepsis, size=int(len(patients_without_sepsis) * test_size), replace=False)\n",
    "\n",
    "    # Combiner les patients de test\n",
    "    test_patients = np.concatenate((test_patients_with_sepsis, test_patients_without_sepsis))\n",
    "\n",
    "    # Créer les ensembles de données\n",
    "    train_df = df[~df['Patient_ID'].isin(test_patients)]\n",
    "    test_df = df[df['Patient_ID'].isin(test_patients)]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def split_train_test_data_v2(df, test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Sépare les données en ensembles d'entraînement et de test, en évitant que les données d'un même patient soient dans les deux ensembles.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    test_size (float): La proportion de chaque groupe de patients à utiliser pour le test.\n",
    "    random_seed (int): La graine pour la génération de nombres aléatoires pour la reproductibilité.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Un tuple contenant deux DataFrames, (train_df, test_df).\n",
    "    \"\"\"\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    # Obtenir les indices des ensembles d'entraînement et de test en groupant par 'Patient_ID'\n",
    "    for train_idx, test_idx in gss.split(df, groups=df['Patient_ID']):\n",
    "        train_df = df.iloc[train_idx]\n",
    "        test_df = df.iloc[test_idx]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "##################### Préparation des données pour les Réseaux de Neurones ########################\n",
    "###################################################################################################\n",
    "def extract_sequences_and_labels(df, patient_col, time_col, feature_cols, label_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Extrait des séquences de caractéristiques et des étiquettes à partir du DataFrame groupé par patient.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Le DataFrame contenant les données des patients.\n",
    "    - patient_col (str): La colonne représentant l'identifiant du patient.\n",
    "    - time_col (str): La colonne représentant l'heure ou le temps.\n",
    "    - feature_cols (list): Les colonnes des caractéristiques à utiliser pour le modèle.\n",
    "    - label_col (str): La colonne du label à prédire.\n",
    "    - sequence_length (int): La longueur de la séquence à extraire (par ex. 6).\n",
    "\n",
    "    Returns:\n",
    "    - sequences (list): Une liste de tableaux numpy représentant les séquences de caractéristiques.\n",
    "    - labels (list): Une liste d'étiquettes associées à chaque séquence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Group by patient to get the data for each patient\n",
    "    grouped = df.groupby(patient_col)\n",
    "\n",
    "    for patient_id, patient_data in grouped:\n",
    "        # Trier les données par temps\n",
    "        patient_data = patient_data.sort_values(by=time_col)\n",
    "\n",
    "        # Extraire les caractéristiques et les labels\n",
    "        features = patient_data[feature_cols].values\n",
    "        label = patient_data[label_col].values\n",
    "\n",
    "        # Créer des séquences de longueur fixe\n",
    "        for i in range(len(patient_data) - sequence_length + 1):\n",
    "            sequences.append(features[i:i+sequence_length])\n",
    "            labels.append(label[i+sequence_length-1])  # Label de la dernière heure de la séquence\n",
    "\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "\n",
    "def normalize_sequences_minmax(sequences):\n",
    "    \"\"\"\n",
    "    Normalise chaque séquence avec MinMax scaling entre 0 et 1.\n",
    "\n",
    "    Args:\n",
    "    - sequences (np.array): Séquences à normaliser (samples, time_steps, features).\n",
    "\n",
    "    Returns:\n",
    "    - normalized_sequences (np.array): Séquences normalisées.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Reshape en 2D pour normaliser chaque feature de chaque séquence\n",
    "    n_samples, n_timesteps, n_features = sequences.shape\n",
    "    reshaped_sequences = sequences.reshape(-1, n_features)\n",
    "\n",
    "    # Appliquer MinMaxScaler\n",
    "    scaled_sequences = scaler.fit_transform(reshaped_sequences)\n",
    "\n",
    "    # Reshape en 3D après normalisation\n",
    "    normalized_sequences = scaled_sequences.reshape(n_samples, n_timesteps, n_features)\n",
    "\n",
    "    return normalized_sequences, scaler\n",
    "\n",
    "\n",
    "def extract_sequences_and_normalize(train_df, test_df, exclude_columns=['Patient_ID', 'Hour', 'SepsisLabel'], drop_column ='will_have_sepsis', label_column='SepsisLabel', id_column='Patient_ID', time_column='Hour', sequence_length=6):\n",
    "    \"\"\"\n",
    "    Prépare les données d'entraînement et de test pour l'extraction de séquences et la normalisation.\n",
    "\n",
    "    Args:\n",
    "        train_df (DataFrame): Jeu de données d'entraînement.\n",
    "        test_df (DataFrame): Jeu de données de test.\n",
    "        exclude_columns (list): Colonnes à exclure de l'analyse des caractéristiques.\n",
    "        label_column (str): Nom de la colonne des labels.\n",
    "        id_column (str): Nom de la colonne identifiant les patients.\n",
    "        time_column (str): Nom de la colonne représentant le temps.\n",
    "        sequence_length (int): Longueur de chaque séquence (en nombre de points temporels).\n",
    "\n",
    "    Returns:\n",
    "        normalize_sequences_train (ndarray): Séquences normalisées pour l'entraînement.\n",
    "        labels_train (ndarray): Labels pour l'entraînement.\n",
    "        normalize_sequences_test (ndarray): Séquences normalisées pour le test.\n",
    "        labels_test (ndarray): Labels pour le test.\n",
    "        scaler (MinMaxScaler): Scaler utilisé pour la normalisation.\n",
    "    \"\"\"\n",
    "    # Supprimer la colonne des labels pour les features\n",
    "    train_df.drop(columns=drop_column, inplace=True)\n",
    "    test_df.drop(columns=drop_column, inplace=True)\n",
    "\n",
    "    # Sélectionner les colonnes de caractéristiques\n",
    "    feature_cols = [col for col in train_df.columns if col not in exclude_columns]\n",
    "\n",
    "    # Extraction des séquences\n",
    "    sequences_train, labels_train = extract_sequences_and_labels(\n",
    "        train_df, id_column, time_column, feature_cols, label_column, sequence_length\n",
    "    )\n",
    "    sequences_test, labels_test = extract_sequences_and_labels(\n",
    "        test_df, id_column, time_column, feature_cols, label_column, sequence_length\n",
    "    )\n",
    "\n",
    "    # Normaliser les séquences d'entraînement et de test avec MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalize_sequences_train = scaler.fit_transform(\n",
    "        sequences_train.reshape(-1, sequences_train.shape[2])\n",
    "    ).reshape(sequences_train.shape)\n",
    "    normalize_sequences_test = scaler.transform(\n",
    "        sequences_test.reshape(-1, sequences_test.shape[2])\n",
    "    ).reshape(sequences_test.shape)\n",
    "\n",
    "    return normalize_sequences_train, labels_train, normalize_sequences_test, labels_test, scaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "########################### Fonctions pour le modèle XGBoost #####################################\n",
    "##################################################################################################\n",
    "def objective(trial, X, y, cv=7):\n",
    "    \"\"\"\n",
    "    Fonction objectif pour l'optimisation d'hyperparamètres avec Optuna.\n",
    "\n",
    "    Args:\n",
    "    trial (optuna.trial): Un essai de Optuna pour suggérer les hyperparamètres.\n",
    "    X (DataFrame): Features du dataset.\n",
    "    y (Series): Étiquettes cibles du dataset.\n",
    "    cv (int): Nombre de plis pour la validation croisée.\n",
    "\n",
    "    Returns:\n",
    "    float: La moyenne des scores de validation croisée pour les hyperparamètres suggérés.\n",
    "    \"\"\"\n",
    "    # Hyperparamètres suggérés par Optuna\n",
    "    learning_rate = trial.suggest_uniform('learning_rate', 0.01, 1)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 15)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 250)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.01, 5)\n",
    "    subsample = trial.suggest_uniform('subsample', 0.01, 1)\n",
    "\n",
    "    # Création et évaluation du modèle\n",
    "    clf = XGBClassifier(learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        n_estimators=n_estimators,\n",
    "                        min_child_weight=min_child_weight,\n",
    "                        gamma=gamma,\n",
    "                        subsample=subsample,\n",
    "                        use_label_encoder=False,\n",
    "                        eval_metric='logloss')\n",
    "\n",
    "    # Calcul du score moyen sur les plis de validation croisée\n",
    "    score = cross_val_score(clf, X, y, cv=cv)\n",
    "\n",
    "    return np.mean(score)\n",
    "\n",
    "\n",
    "def perform_hyperparameter_optimization(X, y, objective, n_trials=50, random_state=42):\n",
    "    \"\"\"\n",
    "    Crée une étude Optuna pour optimiser les hyperparamètres d'un modèle de machine learning.\n",
    "\n",
    "    Args:\n",
    "    X (DataFrame): Les features d'entrée pour le modèle.\n",
    "    y (Series): Les étiquettes cibles.\n",
    "    objective (function): La fonction objective pour Optuna.\n",
    "    n_trials (int): Le nombre de tentatives d'optimisation.\n",
    "    random_state (int): Graine pour la génération de nombres aléatoires pour la reproductibilité.\n",
    "\n",
    "    Returns:\n",
    "    dict: Meilleurs hyperparamètres trouvés par l'étude Optuna.\n",
    "    \"\"\"\n",
    "    # Création d'un objet study d'Optuna\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "\n",
    "    # Lancement de l'optimisation\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=n_trials)\n",
    "\n",
    "    # Retourne les meilleurs paramètres trouvés\n",
    "    return study.best_params, study\n",
    "\n",
    "# sauvegarde des meilleurs hyperparamètres\n",
    "def save_study(study, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "\n",
    "# chargement des meilleurs hyperparamètres\n",
    "def load_study(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        study = pickle.load(f)\n",
    "    return study\n",
    "\n",
    "def train_and_save_xgboost_classifier(X_train, y_train, best_params, model_path):\n",
    "    \"\"\"\n",
    "    Crée, entraîne et sauvegarde un modèle XGBoost avec des paramètres spécifiés.\n",
    "\n",
    "    Args:\n",
    "    X_train (DataFrame): Les features d'entraînement.\n",
    "    y_train (Series): Les étiquettes cibles d'entraînement.\n",
    "    best_params (dict): Dictionnaire contenant les meilleurs paramètres pour le modèle.\n",
    "    model_path (str): Chemin du fichier où le modèle sera sauvegardé.\n",
    "\n",
    "    Returns:\n",
    "    XGBClassifier: Le modèle XGBoost entraîné.\n",
    "    \"\"\"\n",
    "    xgbc = XGBClassifier(\n",
    "        n_jobs=-1,  # Utiliser tous les processeurs disponible\n",
    "        tree_method='hist',  # Utiliser 'hist' pour accélérer l'entraînement\n",
    "        subsample=0.8,       # Sous-échantillonnage pour réduire le temps d'entraînement\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        min_child_weight=best_params['min_child_weight'],\n",
    "        gamma=best_params['gamma'],\n",
    "        #subsample=best_params['subsample'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    xgbc.fit(X_train, y_train)\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    with open(model_path, 'wb') as file:\n",
    "        pickle.dump(xgbc, file)\n",
    "\n",
    "    return xgbc\n",
    "\n",
    "def explain_model_predictions_with_shap(model, X_train, X_test):\n",
    "    \"\"\"\n",
    "    Explique les prédictions d'un modèle XGBoost en utilisant SHAP.\n",
    "\n",
    "    Args:\n",
    "    model (XGBClassifier): Le modèle entraîné.\n",
    "    X_train (DataFrame): Les données d'entraînement.\n",
    "    X_test (DataFrame): Les données de test pour lesquelles les explications sont générées.\n",
    "\n",
    "    Returns:\n",
    "    None: Affiche les graphiques SHAP.\n",
    "    \"\"\"\n",
    "    # Création d'un explainer SHAP\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "    # Calcul des valeurs SHAP\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    # Affichage du summary plot\n",
    "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "\n",
    "def load_xgboost_classifier(model_path):\n",
    "    \"\"\"\n",
    "    Charge un modèle XGBoost sauvegardé à partir d'un fichier.\n",
    "\n",
    "    Args:\n",
    "    model_path (str): Chemin du fichier où le modèle est sauvegardé.\n",
    "\n",
    "    Returns:\n",
    "    XGBClassifier: Le modèle XGBoost chargé.\n",
    "    \"\"\"\n",
    "    with open(model_path, 'rb') as file:\n",
    "        xgbc_loaded = pickle.load(file)\n",
    "\n",
    "    return xgbc_loaded\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "######################### Fonctions d'Evaluation #################################################\n",
    "##################################################################################################\n",
    "def predict_and_evaluate(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Fait des prédictions avec un modèle donné et évalue les résultats.\n",
    "\n",
    "    Args:\n",
    "    model (XGBClassifier): Le modèle entraîné à utiliser pour les prédictions.\n",
    "    X_test (DataFrame): Les features de test.\n",
    "    y_test (Series): Les étiquettes cibles de test.\n",
    "\n",
    "    Returns:\n",
    "    str: Un rapport d'évaluation imprimable.\n",
    "    \"\"\"\n",
    "    # Prédiction avec le modèle\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    # Génération du rapport d'évaluation\n",
    "    evaluation_report = classification_report(y_test, y_predicted)\n",
    "\n",
    "    return evaluation_report\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Évalue les performances du modèle en affichant la précision, recall, accuracy, f1-score,\n",
    "    matrice de confusion, et AUROC.\n",
    "\n",
    "    Args:\n",
    "    - model: Le modèle entraîné.\n",
    "    - X_test: Les données d'entrée de test.\n",
    "    - y_test: Les labels de test.\n",
    "    - threshold: Le seuil de décision pour classer les prédictions (par défaut 0.5).\n",
    "    \"\"\"\n",
    "    # Prédire les probabilités\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "\n",
    "    # Binariser les prédictions en fonction du seuil\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Calculer les métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"AUROC: {auroc:.4f}\")\n",
    "\n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['No Sepsis', 'Sepsis'], yticklabels=['No Sepsis', 'Sepsis'])\n",
    "    plt.title('Matrice de Confusion')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    # Courbe ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', label=f'AUROC = {auroc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred_proba, y_pred\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Évalue le modèle sur les données de test et affiche le rapport de classification et la matrice de confusion.\n",
    "\n",
    "    Args:\n",
    "    model (keras.Model): Le modèle LSTM à évaluer.\n",
    "    X_test (np.array): Données de test.\n",
    "    y_test (np.array): Étiquettes de test.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', cbar=False, annot_kws={\"size\": 16})\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Affiche les courbes de précision et de perte pour les ensembles d'entraînement et de validation.\n",
    "\n",
    "    Args:\n",
    "    - history: Historique de l'entraînement du modèle (history object de Keras).\n",
    "    \"\"\"\n",
    "    # Récupérer les données de l'historique\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "    # Tracé de la précision\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Courbe d'accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, accuracy, 'b-', label='Accuracy entraînement')\n",
    "    plt.plot(epochs, val_accuracy, 'r-', label='Accuracy validation')\n",
    "    plt.title('Accuracy - Entraînement vs Validation')\n",
    "    plt.xlabel('Épochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Courbe de perte (loss)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b-', label='Perte entraînement')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Perte validation')\n",
    "    plt.title('Perte - Entraînement vs Validation')\n",
    "    plt.xlabel('Épochs')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
